{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notebook\n",
    "\n",
    "#### RWS Group 3\n",
    "\n",
    "- Janine Timmerman - 4831578\n",
    "- Honghao Zhao - 5735289\n",
    "- Zhaoyu Yan - 5844940\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important information***\n",
    "\n",
    "This notebook is the final notebook of RWS group 3. This notebook will take you through our process and shows the final results and conclusions. To keep this notebook clear and not too cluttered, not all the details will be presented in this notebook. Each section will refer to other notebooks if needed to show the full code and process. \n",
    "\n",
    "This notebook and all other notebooks important for understanding the process will be available as a .ipynb file which can be run (altough it might take a lot of time), and a html file for easy viewing. All the html version of the notebooks can be found in the folder: **html_notebooks**.\n",
    "\n",
    "The backlog diary can be found in github in the wiki.\n",
    "\n",
    "The dashboard can be accessed with the following link: https://rws-group3-48o79awm3zv6cbmgpeaj2d.streamlit.app/ \n",
    "(Loading the dashboard should not take more than 1 or 2 minutes. We've noticed it doesn't work (quickly) on all computers. Otherwise you could try a different browser/ pc if possible or watch the demo we provide: ***LINK FOR VIDEO SHOWCASING THE DASHBOARD***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* 1 Introduction\n",
    "* 2 Importing modules\n",
    "* 3 Analysis of incident data\n",
    "\n",
    "    * 3.1 Filtering of the data\n",
    "    * 3.2 Analysis of incidents\n",
    "    * 3.3 Splitting training and validation data\n",
    "\n",
    "* 4 Travel times\n",
    "\n",
    "    * 4.1 Speed data on road sections\n",
    "    * 4.2 NetworkX graph creation\n",
    "    * 4.3 Travel time between nodes\n",
    "\n",
    "\n",
    "* 5 Algorithms for inspector locations\n",
    "\n",
    "    * 5.1 Objective functions\n",
    "    * 5.2 K-means clustering by distance\n",
    "    * 5.3 K-means clustering by travel time\n",
    "    * 5.4 Simulated Annealing\n",
    "    * 5.5 Frequency-based\n",
    "    \n",
    "* 6 Validation\n",
    "* 7 Results and conclusions\n",
    "* 8 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each day many incidents happen on the road network of the Netherlands. To minimize the consequences on other traffic, road inspectors are send to the locations of the incidents. To ensure good and safe traffic flow, it is important that the road inspectors reach the locations as fast as possible. For this the road inspectors need to be stationed at efficient places so all incidents locations can be reached as far as possible. Currently, the average travel time of a road inspector to an incident is 18 minutes. The goal of this assignment is to find locations to get a travel time lower than 18 minutes. \n",
    "\n",
    "This gives the following research question:\n",
    "\n",
    "***What are the optimum locations for road inspectors to reach incidents as fast as possible?***\n",
    "\n",
    "To help answer the question, there are several subquestions\n",
    "\n",
    "* What are the best optimisation methods to locate the road inspectors to reach the incidents?\n",
    "* What kind of different scenarios could be classified? (e.g. peak hour situation)\n",
    "* How many inspectors should be introduced to the network in total to ensure that each incident has at least one inspector?\n",
    "\n",
    "The remainder of this notebook will show the locations of inspectors we found and the process that brought us there. First, the data of the incidents is filtered and analysed. After that, data is collected about the speed and intensities of the road network, which is used ot determine the speed and travel times on each road section. After that the different algorithms are presented. The results of these algorithms are then validated and finally, the result and conclusions are shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, many different python modules and libraries were used to make our code more efficient and to showcase the visualizations. There modules are imported in this section.\n",
    "\n",
    "If it is not possible to run the following noteblock (and you want to): you can install the missing modules using\n",
    "\n",
    "        pip install \"NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import namedtuple\n",
    "\n",
    "import networkx as nx\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import folium \n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "import branca.colormap as cm\n",
    "from distinctipy import distinctipy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll\n",
    "\n",
    "import json\n",
    "\n",
    "# Others\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting coordinates\n",
    "\n",
    "def DutchRDtoWGS84(rdX, rdY):\n",
    "    \"\"\" Convert DutchRD to WGS84\n",
    "    \"\"\"\n",
    "    RD_MINIMUM_X = 11000\n",
    "    RD_MAXIMUM_X = 280000\n",
    "    RD_MINIMUM_Y = 300000\n",
    "    RD_MAXIMUM_Y = 630000\n",
    "    if (rdX < RD_MINIMUM_X or rdX > RD_MAXIMUM_X\n",
    "        or rdY < RD_MINIMUM_Y or rdY > RD_MAXIMUM_Y):\n",
    "        resultNorth = -1\n",
    "        resultEast = -1\n",
    "        return resultNorth, resultEast\n",
    "    # else\n",
    "    dX = (rdX - 155000.0) / 100000.0\n",
    "    dY = (rdY - 463000.0) / 100000.0\n",
    "    k = [[3600 * 52.15517440, 3235.65389, -0.24750, -0.06550, 0.0],\n",
    "        [-0.00738   ,   -0.00012,  0.0    ,  0.0    , 0.0],\n",
    "        [-32.58297   ,   -0.84978, -0.01709, -0.00039, 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [0.00530   ,    0.00033,  0.0    ,  0.0    , 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0]]\n",
    "    l = [[3600 * 5.38720621,    0.01199,  0.00022,  0.0    , 0.0],\n",
    "        [5260.52916   ,  105.94684,  2.45656,  0.05594, 0.00128],\n",
    "        [-0.00022   ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [-0.81885   ,   -0.05607, -0.00256,  0.0    , 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [0.00026   ,    0.0    ,  0.0    ,  0.0    , 0.0]]\n",
    "    resultNorth = 0\n",
    "    resultEast = 0\n",
    "    powX = 1\n",
    "\n",
    "    for p in range(6):\n",
    "        powY = 1\n",
    "        for q in range(5):\n",
    "            resultNorth = resultNorth + k[p][q] * powX * powY / 3600.0\n",
    "            resultEast = resultEast + l[p][q] * powX * powY / 3600.0\n",
    "            powY = powY * dY\n",
    "        powX = powX * dX\n",
    "    return resultNorth, resultEast\n",
    "\n",
    "def WGS84toDutchRD(wgs84East, wgs84North):\n",
    "    # translated from Peter Knoppers's code\n",
    "\n",
    "    # wgs84East: longtitude\n",
    "    # wgs84North: latitude\n",
    "\n",
    "    # Western boundary of the Dutch RD system. */\n",
    "    WGS84_WEST_LIMIT = 3.2\n",
    "\n",
    "    # Eastern boundary of the Dutch RD system. */\n",
    "    WGS84_EAST_LIMIT = 7.3\n",
    "\n",
    "    # Northern boundary of the Dutch RD system. */\n",
    "    WGS84_SOUTH_LIMIT = 50.6\n",
    "\n",
    "    # Southern boundary of the Dutch RD system. */\n",
    "    WGS84_NORTH_LIMIT = 53.7\n",
    "\n",
    "    if (wgs84North > WGS84_NORTH_LIMIT) or \\\n",
    "        (wgs84North < WGS84_SOUTH_LIMIT) or \\\n",
    "        (wgs84East < WGS84_WEST_LIMIT) or \\\n",
    "        (wgs84East > WGS84_EAST_LIMIT):\n",
    "        resultX = -1\n",
    "        resultY = -1\n",
    "    else:\n",
    "        r = [[155000.00, 190094.945,   -0.008, -32.391, 0.0],\n",
    "            [-0.705, -11832.228,    0.0  ,   0.608, 0.0],\n",
    "            [0.0  ,   -114.221,    0.0  ,   0.148, 0.0],\n",
    "            [0.0  ,     -2.340,    0.0  ,   0.0  , 0.0],\n",
    "            [0.0  ,      0.0  ,    0.0  ,   0.0  , 0.0]]\n",
    "        s = [[463000.00 ,      0.433, 3638.893,   0.0  ,  0.092],\n",
    "            [309056.544,     -0.032, -157.984,   0.0  , -0.054],\n",
    "            [73.077,      0.0  ,   -6.439,   0.0  ,  0.0],\n",
    "            [59.788,      0.0  ,    0.0  ,   0.0  ,  0.0],\n",
    "            [0.0  ,      0.0  ,    0.0  ,   0.0  ,  0.0]]\n",
    "        resultX = 0\n",
    "        resultY = 0\n",
    "        powNorth = 1\n",
    "        dNorth = 0.36 * (wgs84North - 52.15517440)\n",
    "        dEast = 0.36 * (wgs84East - 5.38720621)\n",
    "\n",
    "        for p in range(5):\n",
    "            powEast = 1\n",
    "            for q in range(5):\n",
    "                resultX = resultX + r[p][q] * powEast * powNorth\n",
    "                resultY = resultY + s[p][q] * powEast * powNorth\n",
    "                powEast = powEast * dEast\n",
    "            powNorth = powNorth * dNorth\n",
    "    return resultX, resultY\n",
    "\n",
    "def calc_distance(line_wkt):\n",
    "    line = ogr.CreateGeometryFromWkt(line_wkt)\n",
    "    points = line.GetPoints()\n",
    "    d = 0\n",
    "    for p0, p1 in zip(points, points[1:]):\n",
    "        d = d + geodesic(p0, p1).m\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis of incident data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will filter and analyse the incident data. After that the data is split for training the algorithms and validating the algorithms.\n",
    "\n",
    "For the full filtering and data analysis see ***NAME NOTEBOOK***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filtering of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can:\n",
    "- (1) Delete the incidents which are not occured on the high ways \n",
    "- (2) Delete the incidents which the values for road number is missing \n",
    "- (3) Change name of all roads 'hrb'\n",
    "- (4) Delete the incidents that are out of the border of the Netherlands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter data\n",
    "def data_filter(data_input):\n",
    "    data_input = incidents.dropna()\n",
    "    data_input.loc[:,'road_number'] = data_input['road_number'].replace({'A12 hrb':'A12', 'A16 hrb':'A16', 'A2 hrb':'A2'})\n",
    "    new_data = data_input[data_input['road_number'].str.startswith('A')]\n",
    "    new_data.drop(new_data.loc[new_data['index'] == 183130].index, inplace=True)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Read raw data from a csv file\n",
    "incidents = pd.read_csv('Dashboard_data\\incidents_data', sep=';')\n",
    "incidents.columns = ['index', 'id', 'type', 'start_time','end_time', 'road_number','longitude','latitude']\n",
    "incidents['start_time'] = pd.to_datetime(incidents['start_time'])\n",
    "incidents['end_time'] = pd.to_datetime(incidents['end_time'])\n",
    "incidents.head()\n",
    "\n",
    "# Dataframe of filtered data\n",
    "incidents_df = data_filter(incidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysis of incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey will explain here more :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Mark all incidents at the road network in heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_incidents(filter_type, keyword):\n",
    "\n",
    "    # Define a new map\n",
    "    m = folium.Map(location=[52.399190, 4.893658])\n",
    "\n",
    "    if filter_type == 'Incident_type':\n",
    "        new_data = incidents_df.loc[incidents_df['type'] == keyword]\n",
    "        # Extract the latitude and longitude as a list of lists\n",
    "        heat_data = [[row['latitude'], row['longitude']] for _, row in new_data.iterrows()]\n",
    "        # Create a heatmap layer\n",
    "        HeatMap(heat_data).add_to(m)\n",
    "    return m\n",
    "\n",
    "map_new = draw_incidents('Incident_type', 'accident')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Analysis of time periods when incidents occurs (starting time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_df['hour_of_day'] = incidents_df['start_time'].apply(lambda x: x.hour)\n",
    "hourly_counts = incidents_df.groupby('hour_of_day').size().reset_index(name='accident_count')\n",
    "\n",
    "# Calculate the probability of time of a day for incident\n",
    "pro_0_5h = hourly_counts['accident_count'][:6].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_6_9h = hourly_counts['accident_count'][6:10].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_10_14h = hourly_counts['accident_count'][10:15].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_15_18h =hourly_counts['accident_count'][15:19].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_19_23h = hourly_counts['accident_count'][19:].sum() / hourly_counts['accident_count'].sum()\n",
    "\n",
    "result_data = {\n",
    "    'Time Range': ['0-5 hours', '6-9 hours', '10-14 hours', '15-18 hours', '19-23 hours'],\n",
    "    'Probability': [pro_0_5h, pro_6_9h, pro_10_14h, pro_15_18h, pro_19_23h]\n",
    "}\n",
    "\n",
    "pro_time = pd.DataFrame(result_data)\n",
    "pro_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Analysis of day of week when incidents occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability of accident of time of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_df['day of week'] = incidents_df ['start_time'].dt.dayofweek\n",
    "weekly_count = incidents_df['day of week'].value_counts(normalize=True)\n",
    "day_mapping = {\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday',\n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "weekly_count.index = weekly_count.index.map(day_mapping)\n",
    "\n",
    "pro_weekly = pd.DataFrame({\n",
    "    'Day of Week': weekly_count.index,\n",
    "    'Probability': weekly_count.values\n",
    "})\n",
    "pro_weekly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Analysis of accident frequency and duration time in each highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of accidents in each road\n",
    "accidents_number = incidents_df.groupby('road_number').size()\n",
    "accidents_df = accidents_number.reset_index()\n",
    "accidents_df.columns = ['road_number', 'accidents_number']\n",
    "\n",
    "# count the average lasting time for each road\n",
    "incidents_df['Duration_time'] = (incidents_df['end_time'] - incidents_df['start_time']).dt.total_seconds() / 60\n",
    "average_duration_by_road = incidents_df.groupby('road_number')['Duration_time'].mean()\n",
    "duration_df = average_duration_by_road.reset_index()\n",
    "duration_df.columns = ['road_number', 'average_duration']\n",
    "\n",
    "# Mix them and create the new dataframe\n",
    "road_number_counts = pd.merge(accidents_df, duration_df, on='road_number', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Splitting training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey will explain here more :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = incidents_df[['latitude','longitude']].values\n",
    "num_samples = len(data)\n",
    "num_samples_keep = int(0.8*num_samples)\n",
    "\n",
    "#Random distruption data\n",
    "np.random.shuffle(data)\n",
    "selected_data = data[:num_samples_keep]\n",
    "remaining_data = data[num_samples_keep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Travel times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm will need to calculate the travel times between inspectors and incidents to find the optimal locations of the road inspectors. All the tools needed to calculate these travel times are given in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Speed data on road sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the speed on each road section of the network is determined. For the full calcutions please take a look at the notebook **speed_network_data.ipynb** or **NAME HTML VERSION**. This section will only show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First there is the following dataframe. It has been obtained by combining the given shapefiles, open source wkd (\"Wegkenmerkendatabase\") data about the maximum speed and INWEVA (\"INtensiteit WEgVAkken\") data which gives traffic intensities on road sections. There was some missing data, which has been filled with data from adjacent road sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_section_data = pd.read_csv('speed_data', sep=';')\n",
    "road_section_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The travel times on each road section have been determined for optimal conditions, which assumed that the road inspector is driving at the maximum speed (in 2019) and for peak hour conditions. The peak hour conditions were estimated using a data-driven function using data like the intensity on the road, time of the day, no. of lanes, etc. (See the speed_network_data notebook for all details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 NetworkX graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, there was some missing data about e.g. the maximum speed on the road sections. This data was filled by looking at the maximum speed of adjacent road sections. This was done by creating a directed NetworkX graph. \n",
    "\n",
    "Beneath, you see the code that was used to create the graph. The start and end point of each edge was added as a node to the network. The road section was then added as an edge between the two nodes. Several attributes were added to the edges to keep track of the properties of the road section.\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    nx.set_edge_attributes(G, 0, 'Max_speed')\n",
    "    nx.set_edge_attributes(G, 0, 'Road_number')\n",
    "    nx.set_edge_attributes(G, 0, 'Road_section_id')\n",
    "\n",
    "    for i in range(len(merged_network)):\n",
    "\n",
    "        # Get the first and last node from each road section\n",
    "        start = merged_network.geometry[i].coords[0]\n",
    "        end = merged_network.geometry[i].coords[-1]\n",
    "\n",
    "        # Add the nodes and edges to the graph\n",
    "        G.add_node(start)\n",
    "        G.add_node(end)\n",
    "\n",
    "        G.add_edge(start, end, geometry=merged_network.geometry[i])\n",
    "\n",
    "        # Add different attributes from the dataframe to the graph\n",
    "        G.edges[(start, end)]['Max_speed'] = merged_network.Max_speed[i]\n",
    "        G.edges[(start, end)]['Road_number'] = merged_network.Road_number[i]\n",
    "        G.edges[(start, end)]['Road_section_id'] = merged_network.Road_section_id[i]\n",
    "\n",
    "Because of NetworkX functions, it was easy to get adjecent edges tp use for filling in missing data. Later on, more attributes were added to the network as more was calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load networkX graph\n",
    "G = pickle.load(open('NetworkX_graph.pickle', 'rb'))\n",
    "\n",
    "# Show edge attributes of random edge\n",
    "G.edges[list(G.edges)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the graph contains speeds for optimal and peak hour conditions, road number, road section id and the travel times.\n",
    "\n",
    "However, when trying to find the path between shortest nodes, it was quickly concluded that this graph is not yet strongly connected. In other words, it was not possible to find a path between each two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_strongly_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to make the graph fully connected, so it is possible to calculate the travel time between all nodes. The full process of connecting all nodes can be found in the following notebook: **connect_networkX_graph.ipynb** or **HTML VERSION**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect the full network, two different algorithms were used.\n",
    "\n",
    "First, there was looped over each node in graph G to check if the node missed an incoming or outgoing node. If one of those were missing, the algorithms would loop over the nearest nodes and check if it complied to several constraints. If so, the nodes were connected. Using this method around 1800 edges were added to the network and all nodes had at least an incoming and an outgoing edge. (So it is possible to reach and leave the node).\n",
    "\n",
    "After that, the network was still not strongly connected and a second algorithm was used. There were still over 400 strongly connected components (instead of 1). The second algorithm looped over all strongly connected components and would connect each component to the nearest component. This algorithm used less constraints and the added edges might be less accurate.\n",
    "\n",
    "Still, after the second algorithm, the whole network was strongly connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load networkX graph\n",
    "G = pickle.load(open('NetworkX_graph_new.pickle', 'rb'))\n",
    "\n",
    "# Check if new network is strongly connected\n",
    "nx.is_strongly_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there only needs to be checked if edges that were added to the network seem realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about when the edge was added (or if it already existed)\n",
    "new_edge_val = np.array(list(nx.get_edge_attributes(G, 'New_edge').values()))\n",
    "new_edge_key = np.array(list(nx.get_edge_attributes(G, 'New_edge').keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_sorted = []\n",
    "\n",
    "for i in range(3):\n",
    "    edges_sorted.append(new_edge_key[new_edge_val == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Road network of the Netherlands')\n",
    "\n",
    "# Give axis equal scale\n",
    "fig.set_figheight(9)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Define colors and labels\n",
    "colors = ['cornflowerblue', 'red', 'green']\n",
    "labels = ['Original edges', 'Edges added using first method',\n",
    "          'Edges added using second method']\n",
    "\n",
    "# Plot the different colored edges in the road network\n",
    "for i in range(3):\n",
    "    ax.plot([],[], color=colors[i], label=labels[i])\n",
    "\n",
    "    for j in range(len(edges_sorted[i])):\n",
    "        ax.plot(edges_sorted[i][j,:,0], \n",
    "                edges_sorted[i][j,:,1], color=colors[i])\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the edges added in the first iteration seem to connect small stretches between nodes.'\n",
    "\n",
    "The green edges are larger, especially around several roads in Zeeland and Drenthe. This is due to the fact these are more regional roads and were given in the shapefile as a single lane, instead of several lanes as was done for the highways. This means according to the networkX graph, the road could only be driven in one direction and that had to be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Travel time between nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the network graph is finished, a function must be defined to calculate travel times. This function is used in all the algorithms to calculate the travel times. For the full notebook of this function see **df.ipynb** or **HTML VERSION**. This notebook also shows several visualizations of shortest paths between nodes and some visualizations about the areas an inspector serves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_time function\n",
    "def travel_time_func(point1, point2, time='min'):\n",
    "    \"\"\"This function uses the information given in network X to return the travel time between two points.\n",
    "        point1 and point2 should be tuples with the coordinates in longitude, latitude.\n",
    "        if time = 'peak', the peak travel time is used. In all other cases the minimum travel time is used.\"\"\"\n",
    "\n",
    "    # Determine which travel times to use\n",
    "    if time == 'peak':\n",
    "        time_string = 'Peak_travel_time_[s]'\n",
    "    else:\n",
    "        time_string = 'Min_travel_time_[s]'\n",
    "\n",
    "    # Change points to Dutch system\n",
    "    p1_x, p1_y = WGS84toDutchRD(point1[0], point1[1]) # inspector\n",
    "    p2_x, p2_y = WGS84toDutchRD(point2[0], point2[1]) # incident\n",
    "\n",
    "    # Create numpy matrix from nodes\n",
    "    A = np.array(list(G.nodes()))\n",
    "\n",
    "    # Get node closest to each point\n",
    "    dist_node1, index_node1 = spatial.KDTree(A).query([p1_x, p1_y])\n",
    "    node1 = (A[index_node1][0], A[index_node1][1])\n",
    "\n",
    "    dist_node2, index_node2 = spatial.KDTree(A).query([p2_x, p2_y])\n",
    "    node2 = (A[index_node2][0], A[index_node2][1])\n",
    "\n",
    "    # Get shortest path between nodes\n",
    "    route = nx.shortest_path(G, node1, node2, time_string)\n",
    "    travel_time = nx.shortest_path_length(G, node1, node2, time_string)\n",
    "\n",
    "    return route, travel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function needs as input the WGS84 coordinates of the inspector (origin) and the incident (destination). It then searches for the node closest to the inspector and the node closest to the incident. After that, the shortest path between those 2 nodes is calculated based, using the travel time in optimal conditions, or if you prefer, the travel time using peak hour conditions. \n",
    "\n",
    "Because the distances between 2 nodes are often short (most of the time it takes only a few seconds to travel between two adjacent nodes), the distance between the actual locations and the nodes is neglected. In the aforementioned notebook, the travel times between the node and each incident was calculated (assuming a speed of 120 km/h and eucledian distance), and on average it took 9.1 seconds, while the median was 3.4 seconds. This is not much time when looking at an average travel time of 18 minutes.\n",
    " \n",
    "(There are several larger outliers however, where the distance between a node and incident are up to 6 minutes. This makes this method less accurate, because the actual travel time between the incident and the inspector could be 6 minutes longer or shorter. But because the average travel time is still many times lower, those outliers will be neglected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithms for inspector locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the 4 different algorithms are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Add explanations about which objective function is used, or some other like that. Anyway, make it clear how the results of the algorithms can be compared.***\n",
    "\n",
    "The objective function is to compare the number of road inspectors within a time limit, given a fixed mean trvael time, the less the road inspectors, the better the method is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 K-means clustering by distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm of K-means clustering by distance uses the incidents data and the network information as the input. \n",
    "\n",
    "The following steps explain how this method work:\n",
    "\n",
    "1 ) The algorithm uses the Euler distance to cluster all the incidents. The number of road inspectors are straightly linked to the number of clusters, one cluster has one road inspector. The initial value of k is define as 120 in this algorithm.\n",
    "\n",
    "2 ) With the obtained cluster centers, find the nearest node of the road network. The NetworkX and a method of kd-tree is used to find the nearest network node for each cluster center. The found nodes are considered as the locations of the road inspectors.\n",
    "\n",
    "3 ) Calculate the mean travel time of this scenario, check if the mean travel time is within the time limit. If so, minus one to the number of clusters (a.k.a the number of road inspectors).\n",
    "\n",
    "4 ) Repeat step 1 to step 3 until the mean travel time exceeds the time limit. Then the scenario before the scenario in which the iteration stop is considered as the optimal scenario.\n",
    "\n",
    "The result of this algorithm is saved to a .html file named \"min_inspector_map.html\", a single run of this algorithm could take around 5 hours. For further studying purpose, the travel time in peak hours are also used to find the optimal locations of road inspectors, the result is also saved to a .html file. The result of locations of road inspectors during peak hours can be refered in 'peak_inspector_map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of converted coordinates\n",
    "converted_coordinates = [DutchRDtoWGS84(rd_x, rd_y) for (rd_x, rd_y) in list(G.nodes)]\n",
    "\n",
    "# Create a Pandas DataFrame from the converted coordinates\n",
    "node_df = pd.DataFrame(converted_coordinates, columns=['Latitude', 'Longitude'])\n",
    "\n",
    "# Display the DataFrame\n",
    "node_df\n",
    "\n",
    "# Define a data structure to store kd-tree nodes\n",
    "KdNode = namedtuple(\"KdNode\", \"point left right\")\n",
    "\n",
    "# Build k-d tree\n",
    "def kdtree(point_list, depth=0):\n",
    "\n",
    "    if not point_list:\n",
    "        return None\n",
    "\n",
    "    k = 2  # 2-dimensional space\n",
    "    axis = depth % k\n",
    "\n",
    "    point_list.sort(key=lambda x: x[axis])\n",
    "    median = len(point_list) // 2\n",
    "\n",
    "    return KdNode(\n",
    "        point=point_list[median],\n",
    "        left=kdtree(point_list[:median], depth + 1),\n",
    "        right=kdtree(point_list[median + 1 :], depth + 1),\n",
    "    )\n",
    "\n",
    "\n",
    "# Find the closest point\n",
    "def closest_node(root, target, depth=0, best=None):\n",
    "    if root is None:\n",
    "        return best\n",
    "\n",
    "    k = 2  # 2-dimensional space\n",
    "    axis = depth % k\n",
    "\n",
    "    next_best = None\n",
    "    next_branch = None\n",
    "\n",
    "    if best is None or (\n",
    "        point_distance(root.point, target) < point_distance(best, target)\n",
    "    ):\n",
    "        next_best = root.point\n",
    "    else:\n",
    "        next_best = best\n",
    "\n",
    "    if target[axis] < root.point[axis]:\n",
    "        next_branch = root.left\n",
    "    else:\n",
    "        next_branch = root.right\n",
    "\n",
    "    return closest_node(next_branch, target, depth + 1, next_best)\n",
    "\n",
    "\n",
    "# Calculate the distance between points\n",
    "def point_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "\n",
    "# Extract points from dataframe\n",
    "points = [(row[0], row[1]) for index, row in node_df.iterrows()]\n",
    "tree = kdtree(points)\n",
    "\n",
    "\n",
    "def clustering_opt(k_value, data, target_travel_time):\n",
    "    # Create a K-Means clustering model\n",
    "    kmeans = KMeans(n_clusters=k_value)\n",
    "\n",
    "    # Fit the model to the latitude and longitude data\n",
    "    locations = data[[\"latitude\", \"longitude\"]].values\n",
    "    kmeans.fit(locations)\n",
    "\n",
    "    # Assign cluster labels to data points\n",
    "    data[\"cluster\"] = kmeans.labels_ + 1\n",
    "\n",
    "    m = folium.Map(\n",
    "        location=[np.mean(locations[:, 0]), np.mean(locations[:, 1])],\n",
    "        zoom_start=8,\n",
    "        zoom_control=False,\n",
    "    )\n",
    "\n",
    "    # Create a MarkerCluster layer for clustered data\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    # Create a dictionary to keep track of the event count in each cluster\n",
    "    cluster_event_count = {}\n",
    "\n",
    "    # Create an array to store node coordinates for each cluster\n",
    "    cluster_coords = []\n",
    "\n",
    "    # Initiate the longest travel time\n",
    "    longest_travel_time = -1  # second\n",
    "\n",
    "    # Initiate the total travel time\n",
    "    total_travel_time = -1  # second\n",
    "\n",
    "    # Initiate the point count\n",
    "    point_count = 0\n",
    "\n",
    "    # Add markers for clustering\n",
    "    for cluster_label in range(1, k_value + 1):\n",
    "        cluster_data = data[data[\"cluster\"] == cluster_label]\n",
    "        cluster_count = len(cluster_data)\n",
    "        cluster_event_count[cluster_label] = cluster_count\n",
    "        cluster_coords.append(cluster_data)\n",
    "\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            popup_text = f\"Cluster: {row['cluster']}<br>Type: {row['type']}<br>Index: {row['index']}\"\n",
    "            folium.Marker(\n",
    "                [row[\"latitude\"], row[\"longitude\"]], icon=None, popup=popup_text\n",
    "            ).add_to(marker_cluster)\n",
    "\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    ins_loc = [closest_node(tree, coord) for coord in cluster_centers]\n",
    "\n",
    "    for i, center in enumerate(ins_loc):\n",
    "        cluster_label = i + 1\n",
    "        center_popup_text = f\"Cluster_No.{i + 1}<br>Event_Count:{cluster_event_count[cluster_label]} \\\n",
    "            <br>Coordinate:{[format(center[0], '3f'), format(center[1], '.3f')]}\"\n",
    "\n",
    "        # Use ClickForMarker to display the event count when clicking the cluster center\n",
    "        folium.ClickForMarker(popup=center_popup_text).add_to(m)\n",
    "\n",
    "        folium.Marker(\n",
    "            [center[0], center[1]],\n",
    "            icon=folium.Icon(color=\"red\"),\n",
    "            popup=center_popup_text,\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Initialize a list to store time-related information\n",
    "    time_data = []\n",
    "\n",
    "    # Calculate and update the longest travel time in each cluster\n",
    "    for i, center in enumerate(ins_loc):\n",
    "        cluster_label = i + 1\n",
    "        cluster_data = data[data[\"cluster\"] == cluster_label]\n",
    "        point1 = (ins_loc[i][1], ins_loc[i][0])\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            point2 = (row[\"longitude\"], row[\"latitude\"])\n",
    "            r, time_this = travel_time_func(point1, point2, time='min')\n",
    "            total_travel_time += time_this\n",
    "\n",
    "            if time_this > target_travel_time:\n",
    "                point_count += 1\n",
    "\n",
    "            # Store time-related information in the time_data list\n",
    "            time_data.append(\n",
    "                [\n",
    "                    row[\"index\"],\n",
    "                    row[\"id\"],\n",
    "                    row[\"type\"],\n",
    "                    row[\"longitude\"],\n",
    "                    row[\"latitude\"],\n",
    "                    cluster_label,\n",
    "                    time_this,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # Create a Pandas DataFrame to store time-related information\n",
    "    time_df = pd.DataFrame(\n",
    "        time_data,\n",
    "        columns=[\n",
    "            \"index\",\n",
    "            \"id\",\n",
    "            \"type\",\n",
    "            \"longitude\",\n",
    "            \"latitude\",\n",
    "            \"cluster\",\n",
    "            \"time_this\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Calculat the mean travel time\n",
    "    incident_count = data.shape[0]\n",
    "    mean_travel_time = total_travel_time / incident_count\n",
    "\n",
    "    # Print the longest travel time of the scenario with specific number of inspectors\n",
    "    print(f\"The mean travel time is {mean_travel_time} seconds!\")\n",
    "\n",
    "    return m, ins_loc, mean_travel_time, point_count, time_df\n",
    "\n",
    "\n",
    "def find_optimal(initial_k_value, target_travel_time):\n",
    "    # The unit of travel time is second\n",
    "    k_value = initial_k_value\n",
    "\n",
    "    # Initialize the travel time\n",
    "    flag = True\n",
    "\n",
    "    # Iterate to find the optimal number\n",
    "    while True:\n",
    "        if flag == False:\n",
    "            # Keep a record for the previous optimised results\n",
    "            opt_clustered_map = clustered_map\n",
    "            opt_inspector_locations = inspector_locations\n",
    "            opt_mean_travel_time = mean_travel_time\n",
    "            travel_times = time\n",
    "\n",
    "        # Calculate the travel time and the clustered map\n",
    "        (\n",
    "            clustered_map,\n",
    "            inspector_locations,\n",
    "            mean_travel_time,\n",
    "            point_count,\n",
    "            time,\n",
    "        ) = clustering_opt(k_value, incidents_df, target_travel_time)\n",
    "        flag = False\n",
    "        k_value -= 1\n",
    "        if mean_travel_time > target_travel_time:\n",
    "            break\n",
    "\n",
    "    optimal_number = k_value + 1\n",
    "    print(f\"The optimal number of inspectors is {optimal_number}!\\n\")\n",
    "\n",
    "    return (\n",
    "        opt_clustered_map,\n",
    "        opt_inspector_locations,\n",
    "        opt_mean_travel_time,\n",
    "        optimal_number,\n",
    "        travel_times,\n",
    "    )\n",
    "\n",
    "\n",
    "# Use the function to calculate the optimal locations of road inspectors\n",
    "(\n",
    "    inspector_map,\n",
    "    inspector_locations,\n",
    "    opt_mean_travel_time,\n",
    "    optimal_number,\n",
    "    travel_times,\n",
    ") = find_optimal(50, 1080)\n",
    "\n",
    "# The result is saved to a .html file, a single run could take around 5 hours. Here the .html is read to illustarte the result.\n",
    "# The codes below save the result to a .html file named \"min_inspector_map.html\"\n",
    "inspector_map.save(\"min_inspector_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 K-means clustering by travel time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey wil explain here more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we select 80% incident data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = incidents_df[['latitude','longitude']].values\n",
    "num_samples = len(data)\n",
    "num_samples_keep = int(0.8*num_samples)\n",
    "\n",
    "#Random distruption data\n",
    "np.random.shuffle(data)\n",
    "selected_data = data[:num_samples_keep]\n",
    "remaining_data = data[num_samples_keep:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use K means method and iterate to get the optimization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree  # Using cKDTree from scipy\n",
    "import pandas as pd\n",
    "\n",
    "# Build KD Tree\n",
    "def build_kd_tree(node_data):\n",
    "    return cKDTree(node_data)\n",
    "\n",
    "# Find the closest point\n",
    "def closest_node(kdtree, target):\n",
    "    _, nearest_node_index = kdtree.query(target, k=1)\n",
    "    return kdtree.data[nearest_node_index]\n",
    "\n",
    "# Custom K-means function\n",
    "def custom_kmeans_with_travel_time(data, k, max_iterations=5):\n",
    "    # Build KD Tree\n",
    "    kdtree = build_kd_tree(node_data)\n",
    "\n",
    "    # Initialize K-means\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=max_iterations, n_init=1, random_state=0)\n",
    "\n",
    "    # Fit K-means to get initial cluster centers\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # Initialize cluster_centers with initial cluster centers\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_centers = cluster_centers[:, [1, 0]]\n",
    "\n",
    "    max_travel_time = 18 * 60  # 18 minutes in seconds\n",
    "\n",
    "    # Continue reassigning incidents until no reassignments are made (Here is the main difference from the first method!)\n",
    "    reassignments_made = True\n",
    "    while reassignments_made:\n",
    "        reassignments_made = False\n",
    "        for i in range(k):\n",
    "            cluster_center = cluster_centers[i]\n",
    "            cluster_incidents = data[kmeans.labels_ == i]\n",
    "            cluster_incidents = cluster_incidents[:, [1, 0]]\n",
    "\n",
    "            for incident in cluster_incidents:\n",
    "                _, travel_time = travel_time_func(incident, cluster_center)\n",
    "                if travel_time > max_travel_time:\n",
    "                    # Find the nearest node using KD Tree\n",
    "                    nearest_node = closest_node(kdtree, incident)\n",
    "\n",
    "                    # Reassign the incident to the nearest cluster\n",
    "                    cluster_centers[i] = nearest_node\n",
    "                    reassignments_made = True\n",
    "                    break\n",
    "            \n",
    "        print(f'cluster center {i} is {cluster_centers}')          \n",
    "\n",
    "    return kmeans.labels_, cluster_centers\n",
    "\n",
    "# Use the new KD Tree library\n",
    "# k = 47\n",
    "# data = selected_data\n",
    "# node_data = node_df[['longitude', 'latitude']].values\n",
    "# cluster_labels, cluster_centers = custom_kmeans_with_travel_time(data, k, max_iterations=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Frequency-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey will explain here more :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will show the results from the validation, and thus shows how good the algorithms perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Read and input the inspectors coordinates of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('Joey result1.csv')\n",
    "\n",
    "latitude = result_df['latitude'].values\n",
    "longitude = result_df['longitude'].values\n",
    "\n",
    "latitude = np.array([float(lat) for lat in latitude])\n",
    "longitude = np.array([float(lon) for lon in longitude])\n",
    "\n",
    "inspector_coordinates = np.column_stack((longitude, latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Here we calculate the shorest travel time of each incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = remaining_data\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each incident\n",
    "for i, incident_coord in enumerate(validation_data):\n",
    "    minimum_travel_time = float('inf')\n",
    "    \n",
    "    # Loop through each inspector's coordinates\n",
    "    for j, inspector_coord in enumerate(inspector_coordinates):\n",
    "        _, travel_time = travel_time_func(inspector_coord, incident_coord)\n",
    "        if travel_time < minimum_travel_time:\n",
    "            minimum_travel_time = travel_time\n",
    "    \n",
    "    results.append([i, minimum_travel_time])\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results, columns=[\"Incident Index\", \"Minimum Travel Time\"])\n",
    "start_time_column = pd.DataFrame(validation_dataframe['start_time'].values, columns=['start_time'])\n",
    "longitude_column = pd.DataFrame(validation_dataframe['longitude'].values, columns=['longitude'])\n",
    "latitude_column = pd.DataFrame(validation_dataframe['latitude'].values, columns=['latitude'])\n",
    "results_df['start_time'] = start_time_column\n",
    "results_df['longitude'] = longitude_column\n",
    "results_df['latitude'] = latitude_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Validation in time dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we classify the data by day of week and time of a day, and then calculate the average travel time they need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = results_df['start_time'].dt.dayofweek\n",
    "results_df['day of week'] = day_of_week\n",
    "results_df['hour of day'] = results_df['start_time'].apply(lambda x: x.hour)\n",
    "\n",
    "results = []\n",
    "time_ranges = [(0, 5), (6, 9), (10, 14), (15, 18), (19, 23)]\n",
    "for i in range(7):\n",
    "    data_filter = results_df[results_df['day of week'] == i]\n",
    "\n",
    "    for start_hour, end_hour in time_ranges:\n",
    "        data = data_filter[data_filter['hour of day'].between(start_hour, end_hour)][['Minimum Travel Time']]\n",
    "        \n",
    "        # Calculate the probability that travel time is less than 18 minutes\n",
    "        count_below_18mins = (data['Minimum Travel Time'] < 1080).sum()\n",
    "\n",
    "\n",
    "        if len(data) > 0:\n",
    "            pro_below_18mins = count_below_18mins / len(data)\n",
    "            pro_below_18mins = \"{:.2f}\".format(pro_below_18mins)\n",
    "\n",
    "        else:\n",
    "            pro_below_18mins = 'NAN'\n",
    "\n",
    "        # Calculate the average travel time\n",
    "        avg_travel_time = data['Minimum Travel Time'].mean()\n",
    "\n",
    "\n",
    "        if pd.notna(avg_travel_time):\n",
    "            avg_travel_time = \"{:.2f}\".format(avg_travel_time)\n",
    "        else:\n",
    "            avg_travel_time = 'NAN'\n",
    "\n",
    "        number_of_incidents = len(data)\n",
    "        results.append({\n",
    "            'Day': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][i],\n",
    "            'Time ranges': f'{start_hour}-{end_hour}',\n",
    "            'Number of incidents': number_of_incidents,\n",
    "            'Average Travel Time': avg_travel_time,\n",
    "            'Probability of <18 mins': pro_below_18mins\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Prepare the data\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "time_ranges = ['0-5', '6-9', '10-14', '15-18', '19-23']\n",
    "\n",
    "# Sample data for demonstration purposes\n",
    "Z = np.random.rand(len(time_ranges), len(days))  \n",
    "colors = np.random.rand(len(time_ranges), len(days))  \n",
    "\n",
    "# Create a surface plot\n",
    "surface = go.Surface(z=Z, x=days, y=time_ranges, colorscale='Viridis', cmin=0, cmax=1, colorbar=dict(title='Probability of <18 mins'))\n",
    "\n",
    "# Set axis labels\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Day',\n",
    "        yaxis_title='Time ranges',\n",
    "        zaxis_title='Average Travel Time'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set custom x-axis labels\n",
    "layout.scene.xaxis.update(tickvals=days, ticktext=days)\n",
    "\n",
    "# Set custom y-axis labels\n",
    "layout.scene.yaxis.update(tickvals=time_ranges, ticktext=time_ranges)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[surface], layout=layout)\n",
    "\n",
    "# Set subplot title\n",
    "fig.update_layout(title='Validation Heatmap')\n",
    "\n",
    "# Save the interactive plot as an HTML file\n",
    "\n",
    "# pyo.plot(fig, filename='Time dimension.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Validation in Geographic dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "m = folium.Map(location=[52.399190, 4.893658], zoom_start=10, zoom_control=False)\n",
    "\n",
    "# Create a HeatMap layer to visualize Minimum Travel Time\n",
    "heat_data = [[row['latitude'], row['longitude'], row['Minimum Travel Time']] for _, row in results_df.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "color_gradient = {\n",
    "    0.0: 'blue',\n",
    "    500.0: 'green',\n",
    "    1000.0: 'yellow',\n",
    "    2000.0: 'red'\n",
    "}\n",
    "\n",
    "HeatMap(heat_data, gradient=color_gradient).add_to(m)\n",
    "\n",
    "# m.save('Geographic_dimension.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Show which algorithm is best and why***\n",
    "\n",
    "***Link again to dashboard for more results*** Also refer to notebook(s) that where used to make dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What could have been done better?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For the K-means method, the clusters are based on the Euler distance. If the clusters can use the on-road travel distance, the locations of road inspectors could be more optimised. This is the next-step optimising direction of the K-means method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
