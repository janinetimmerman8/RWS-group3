{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final notebook\n",
    "\n",
    "#### RWS Group 3\n",
    "\n",
    "- Janine Timmerman - 4831578\n",
    "- Honghao Zhao - 5735289\n",
    "- Zhaoyu Yan - 5844940\n",

    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important information***\n",
    "\n",
    "This notebook is the final notebook of RWS group 3. This notebook will take you through our process and shows the final results and conclusions. To keep this notebook clear and not too cluttered, not all the details will be presented in this notebook. Each section will refer to other notebooks if needed to show the full code and process. \n",
    "\n",
    "This notebook and all other notebooks important for understanding the process will be available as a .ipynb file which can be run (altough it might take a lot of time), and a html file for easy viewing. All the html version of the notebooks can be found in the folder: **Notebooks_html**.\n",
    "\n",
    "The backlog diary can be found in github in the wiki.\n",
    "\n",
    "The dashboard can be accessed with the following link: https://rws-group3-48o79awm3zv6cbmgpeaj2d.streamlit.app/ \n",
    "(Loading the dashboard should not take more than 1 or 2 minutes. We've noticed it doesn't work (quickly) on all computers. Otherwise you could try a different browser/ pc if possible or watch the demo we provide: ***LINK FOR VIDEO SHOWCASING THE DASHBOARD***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* 1 Introduction\n",
    "* 2 Importing modules\n",
    "* 3 Analysis of incident data\n",
    "\n",
    "    * 3.1 Filtering of the data\n",
    "    * 3.2 Analysis of incidents\n",
    "    * 3.3 Splitting training and validation data\n",
    "\n",
    "* 4 Travel times\n",
    "\n",
    "    * 4.1 Speed data on road sections\n",
    "    * 4.2 NetworkX graph creation\n",
    "    * 4.3 Travel time between nodes\n",
    "\n",
    "\n",
    "* 5 Algorithms for inspector locations\n",
    "\n",
    "    * 5.1 Objective functions\n",
    "    * 5.2 K-means clustering by distance\n",
    "    * 5.3 K-means clustering by travel time\n",
    "    * 5.4 Simulated Annealing\n",
    "    * 5.5 Frequency-based\n",
    "    \n",
    "* 6 Validation\n",
    "* 7 Results and conclusions\n",
    "* 8 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each day many incidents happen on the road network of the Netherlands. To minimize the consequences on other traffic, road inspectors are send to the locations of the incidents. To ensure good and safe traffic flow, it is important that the road inspectors reach the locations as fast as possible. For this the road inspectors need to be stationed at efficient places so all incidents locations can be reached as far as possible. Currently, the average travel time of a road inspector to an incident is 18 minutes. The goal of this assignment is to find locations to get a travel time lower than 18 minutes. \n",
    "\n",
    "This gives the following research question:\n",
    "\n",
    "***What are the optimum locations for road inspectors to reach incidents as fast as possible?***\n",
    "\n",
    "To help answer the question, there are several subquestions\n",
    "\n",
    "* What are the best optimisation methods to locate the road inspectors to reach the incidents?\n",
    "* What kind of different scenarios could be classified? (e.g. peak hour situation)\n",
    "* How many inspectors should be introduced to the network in total to ensure that each incident has at least one inspector?\n",
    "\n",
    "The remainder of this notebook will show the locations of inspectors we found and the process that brought us there. First, the data of the incidents is filtered and analysed. After that, data is collected about the speed and intensities of the road network, which is used ot determine the speed and travel times on each road section. After that the different algorithms are presented. The results of these algorithms are then validated and finally, the result and conclusions are shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, many different python modules and libraries were used to make our code more efficient and to showcase the visualizations. There modules are imported in this section.\n",
    "\n",
    "If it is not possible to run the following noteblock (and you want to): you can install the missing modules using\n",
    "\n",
    "        pip install \"NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import namedtuple\n",
    "\n",
    "import networkx as nx\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import folium \n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "import branca.colormap as cm\n",
    "from distinctipy import distinctipy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll\n",
    "\n",
    "import json\n",
    "\n",
    "# Others\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting coordinates\n",
    "\n",
    "def DutchRDtoWGS84(rdX, rdY):\n",
    "    \"\"\" Convert DutchRD to WGS84\n",
    "    \"\"\"\n",
    "    RD_MINIMUM_X = 11000\n",
    "    RD_MAXIMUM_X = 280000\n",
    "    RD_MINIMUM_Y = 300000\n",
    "    RD_MAXIMUM_Y = 630000\n",
    "    if (rdX < RD_MINIMUM_X or rdX > RD_MAXIMUM_X\n",
    "        or rdY < RD_MINIMUM_Y or rdY > RD_MAXIMUM_Y):\n",
    "        resultNorth = -1\n",
    "        resultEast = -1\n",
    "        return resultNorth, resultEast\n",
    "    # else\n",
    "    dX = (rdX - 155000.0) / 100000.0\n",
    "    dY = (rdY - 463000.0) / 100000.0\n",
    "    k = [[3600 * 52.15517440, 3235.65389, -0.24750, -0.06550, 0.0],\n",
    "        [-0.00738   ,   -0.00012,  0.0    ,  0.0    , 0.0],\n",
    "        [-32.58297   ,   -0.84978, -0.01709, -0.00039, 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [0.00530   ,    0.00033,  0.0    ,  0.0    , 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0]]\n",
    "    l = [[3600 * 5.38720621,    0.01199,  0.00022,  0.0    , 0.0],\n",
    "        [5260.52916   ,  105.94684,  2.45656,  0.05594, 0.00128],\n",
    "        [-0.00022   ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [-0.81885   ,   -0.05607, -0.00256,  0.0    , 0.0],\n",
    "        [0.0       ,    0.0    ,  0.0    ,  0.0    , 0.0],\n",
    "        [0.00026   ,    0.0    ,  0.0    ,  0.0    , 0.0]]\n",
    "    resultNorth = 0\n",
    "    resultEast = 0\n",
    "    powX = 1\n",
    "\n",
    "    for p in range(6):\n",
    "        powY = 1\n",
    "        for q in range(5):\n",
    "            resultNorth = resultNorth + k[p][q] * powX * powY / 3600.0\n",
    "            resultEast = resultEast + l[p][q] * powX * powY / 3600.0\n",
    "            powY = powY * dY\n",
    "        powX = powX * dX\n",
    "    return resultNorth, resultEast\n",
    "\n",
    "def WGS84toDutchRD(wgs84East, wgs84North):\n",
    "    # translated from Peter Knoppers's code\n",
    "\n",
    "    # wgs84East: longtitude\n",
    "    # wgs84North: latitude\n",
    "\n",
    "    # Western boundary of the Dutch RD system. */\n",
    "    WGS84_WEST_LIMIT = 3.2\n",
    "\n",
    "    # Eastern boundary of the Dutch RD system. */\n",
    "    WGS84_EAST_LIMIT = 7.3\n",
    "\n",
    "    # Northern boundary of the Dutch RD system. */\n",
    "    WGS84_SOUTH_LIMIT = 50.6\n",
    "\n",
    "    # Southern boundary of the Dutch RD system. */\n",
    "    WGS84_NORTH_LIMIT = 53.7\n",
    "\n",
    "    if (wgs84North > WGS84_NORTH_LIMIT) or \\\n",
    "        (wgs84North < WGS84_SOUTH_LIMIT) or \\\n",
    "        (wgs84East < WGS84_WEST_LIMIT) or \\\n",
    "        (wgs84East > WGS84_EAST_LIMIT):\n",
    "        resultX = -1\n",
    "        resultY = -1\n",
    "    else:\n",
    "        r = [[155000.00, 190094.945,   -0.008, -32.391, 0.0],\n",
    "            [-0.705, -11832.228,    0.0  ,   0.608, 0.0],\n",
    "            [0.0  ,   -114.221,    0.0  ,   0.148, 0.0],\n",
    "            [0.0  ,     -2.340,    0.0  ,   0.0  , 0.0],\n",
    "            [0.0  ,      0.0  ,    0.0  ,   0.0  , 0.0]]\n",
    "        s = [[463000.00 ,      0.433, 3638.893,   0.0  ,  0.092],\n",
    "            [309056.544,     -0.032, -157.984,   0.0  , -0.054],\n",
    "            [73.077,      0.0  ,   -6.439,   0.0  ,  0.0],\n",
    "            [59.788,      0.0  ,    0.0  ,   0.0  ,  0.0],\n",
    "            [0.0  ,      0.0  ,    0.0  ,   0.0  ,  0.0]]\n",
    "        resultX = 0\n",
    "        resultY = 0\n",
    "        powNorth = 1\n",
    "        dNorth = 0.36 * (wgs84North - 52.15517440)\n",
    "        dEast = 0.36 * (wgs84East - 5.38720621)\n",
    "\n",
    "        for p in range(5):\n",
    "            powEast = 1\n",
    "            for q in range(5):\n",
    "                resultX = resultX + r[p][q] * powEast * powNorth\n",
    "                resultY = resultY + s[p][q] * powEast * powNorth\n",
    "                powEast = powEast * dEast\n",
    "            powNorth = powNorth * dNorth\n",
    "    return resultX, resultY\n",
    "\n",
    "def calc_distance(line_wkt):\n",
    "    line = ogr.CreateGeometryFromWkt(line_wkt)\n",
    "    points = line.GetPoints()\n",
    "    d = 0\n",
    "    for p0, p1 in zip(points, points[1:]):\n",
    "        d = d + geodesic(p0, p1).m\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis of incident data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will filter and analyse the incident data. After that the data is split for training the algorithms and validating the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filtering of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can:\n",
    "- Delete the incidents which are not occured on the high ways \n",
    "- Delete the incidents which the values for road number is missing \n",
    "- Change name of all roads 'hrb'\n",
    "- Delete the incidents that are out of the border of the Netherlands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter data\n",
    "def data_filter(data_input):\n",
    "    data_input = incidents.dropna()\n",
    "    data_input.loc[:,'road_number'] = data_input['road_number'].replace({'A12 hrb':'A12', 'A16 hrb':'A16', 'A2 hrb':'A2'})\n",
    "    new_data = data_input[data_input['road_number'].str.startswith('A')]\n",
    "    new_data.drop(new_data.loc[new_data['index'] == 183130].index, inplace=True)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Read raw data from a csv file\n",
    "incidents = pd.read_csv('Dashboard_data\\incidents_data', sep=';')\n",
    "incidents.columns = ['index', 'id', 'type', 'start_time','end_time', 'road_number','longitude','latitude']\n",
    "incidents['start_time'] = pd.to_datetime(incidents['start_time'])\n",
    "incidents['end_time'] = pd.to_datetime(incidents['end_time'])\n",
    "incidents.head()\n",
    "\n",
    "# Dataframe of filtered data\n",
    "incidents_df = data_filter(incidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysis of incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a deeper understanding of incidents data, it is necessary to conduct a prelimintary data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Mark all incidents at the road network in heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, there is need to have a  macro-level understanding of the locations of all incients.Here all incidents are plotted at the heat map based on the longitude and langitude coordinates. In this way, it is easy to find the incident high-frequency area. Specicically, the number of incidents occuring on highways near Amsterdam and Rotterdam-Hague area is relatively high, which can be conclued that the probability there is higher thann other area, therefore, it is necessary to assign more inspector there.\n",
    "\n",
    "For heatmap drawn, please see **'3.2 Incident heatmap.html'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_incidents(filter_type, keyword):\n",
    "\n",
    "    # Define a new map\n",
    "    m = folium.Map(location=[52.399190, 4.893658])\n",
    "\n",
    "    if filter_type == 'Incident_type':\n",
    "        new_data = incidents_df.loc[incidents_df['type'] == keyword]\n",
    "        # Extract the latitude and longitude as a list of lists\n",
    "        heat_data = [[row['latitude'], row['longitude']] for _, row in new_data.iterrows()]\n",
    "        # Create a heatmap layer\n",
    "        HeatMap(heat_data).add_to(m)\n",
    "    return m\n",
    "\n",
    "map_new = draw_incidents('Incident_type', 'accident')\n",
    "map_new.save('3.2 Incident heatmap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Analysis of time periods when incidents occurs (starting time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, all incident can be classified by occuring timw, here they are classfied by 5 time ranges, 0-5h(Midnight), 6-9h(morning peak hour), 10-14h(noon), 15-18h(evening peak hour), 19-23h(evening).\n",
    "Then the probability occuring different time range can be obatined.\n",
    "The probability result and bar chart can be shown in **'3.2.2 Probability of different time periods.csv'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_df['hour_of_day'] = incidents_df['start_time'].apply(lambda x: x.hour)\n",
    "hourly_counts = incidents_df.groupby('hour_of_day').size().reset_index(name='accident_count')\n",
    "\n",
    "# Calculate the probability of time of a day for incident\n",
    "pro_0_5h = hourly_counts['accident_count'][:6].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_6_9h = hourly_counts['accident_count'][6:10].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_10_14h = hourly_counts['accident_count'][10:15].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_15_18h =hourly_counts['accident_count'][15:19].sum() / hourly_counts['accident_count'].sum()\n",
    "pro_19_23h = hourly_counts['accident_count'][19:].sum() / hourly_counts['accident_count'].sum()\n",
    "\n",
    "result_data = {\n",
    "    'Time Range': ['0-5 hours', '6-9 hours', '10-14 hours', '15-18 hours', '19-23 hours'],\n",
    "    'Probability': [pro_0_5h, pro_6_9h, pro_10_14h, pro_15_18h, pro_19_23h]\n",
    "}\n",
    "\n",
    "pro_time = pd.DataFrame(result_data)\n",
    "pro_time.to_csv('3.2.2 Probability of different time periods.csv')\n",
    "\n",
    "# Next is the visualiztion of the result as bar chart\n",
    "incidents_df['hour_of_day'] = incidents_df['start_time'].apply(lambda x: x.hour)\n",
    "hourly_counts = incidents_df.groupby('hour_of_day').size().reset_index(name='accident_count')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(hourly_counts['hour_of_day'], hourly_counts['accident_count'])\n",
    "plt.xlabel('Hour of a day')\n",
    "plt.ylabel('Accidents count')\n",
    "plt.title('Accident count by hour of day')\n",
    "plt.xticks(hourly_counts['hour_of_day'])\n",
    "\n",
    "plt.axvspan(0, 5, alpha=0.2, color='red', label='0-5h')\n",
    "plt.axvspan(6, 9, alpha=0.2, color='black', label='6-9h')\n",
    "plt.axvspan(10, 14, alpha=0.2, color='green', label='10-14h')\n",
    "plt.axvspan(15, 18, alpha=0.2, color='yellow', label='15-18h')\n",
    "plt.axvspan(19, 23, alpha=0.2, color='orange', label='19-23h')\n",
    "plt.legend()\n",
    "# plt.show() The figure can be show in the same csv file ''3.2.2 Probability of different time periods.csv''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Analysis of day of week when incidents occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the probabilites of incident occuring different day if a week are also shown.\n",
    "The result can be shown in **'3.2.3 Probability at different days of a week'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_df['day of week'] = incidents_df ['start_time'].dt.dayofweek\n",
    "weekly_count = incidents_df['day of week'].value_counts(normalize=True)\n",
    "day_mapping = {\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday',\n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "weekly_count.index = weekly_count.index.map(day_mapping)\n",
    "\n",
    "pro_weekly = pd.DataFrame({\n",
    "    'Day of Week': weekly_count.index,\n",
    "    'Probability': weekly_count.values\n",
    "})\n",
    "pro_weekly.to_csv('3.2.3 Probability at different days of a week',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Analysis of accident frequency and duration time in each highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows probability of incidents occuring on various highways, along with the corresponding duration times for these incidents.\n",
    "\n",
    "For result, see **3.2.4 Road number count and duration.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of accidents in each road\n",
    "accidents_number = incidents_df.groupby('road_number').size()\n",
    "accidents_df = accidents_number.reset_index()\n",
    "accidents_df.columns = ['road_number', 'accidents_number']\n",
    "\n",
    "# count the average lasting time for each road\n",
    "incidents_df['Duration_time'] = (incidents_df['end_time'] - incidents_df['start_time']).dt.total_seconds() / 60\n",
    "average_duration_by_road = incidents_df.groupby('road_number')['Duration_time'].mean()\n",
    "duration_df = average_duration_by_road.reset_index()\n",
    "duration_df.columns = ['road_number', 'average_duration']\n",
    "\n",
    "# Mix them and create the new dataframe\n",
    "road_number_counts = pd.merge(accidents_df, duration_df, on='road_number', how='left')\n",
    "road_number_counts.to_csv('3.2.4 Road number count and duration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Splitting training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all incident data is classified as two parts: training data(80%) and validation data(20%).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = incidents_df[['latitude','longitude']].values\n",
    "num_samples = len(data)\n",
    "num_samples_keep = int(0.8*num_samples)\n",
    "\n",
    "#Random distruption data\n",
    "np.random.shuffle(data)\n",
    "selected_data = data[:num_samples_keep] # Here we name the training data as seleced_data\n",
    "remaining_data = data[num_samples_keep:] # Here we name the validation data as remaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Travel times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm will need to calculate the travel times between inspectors and incidents to find the optimal locations of the road inspectors. All the tools needed to calculate these travel times are given in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Speed data on road sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the speed on each road section of the network is determined. For the full calcutions please take a look at the notebook **speed_network_data.ipynb** or **NAME HTML VERSION**. This section will only show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First there is the following dataframe. It has been obtained by combining the given shapefiles, open source wkd (\"Wegkenmerkendatabase\") data about the maximum speed and INWEVA (\"INtensiteit WEgVAkken\") data which gives traffic intensities on road sections. There was some missing data, which has been filled with data from adjacent road sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_section_data = pd.read_csv('speed_data', sep=';')\n",
    "road_section_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The travel times on each road section have been determined for optimal conditions, which assumed that the road inspector is driving at the maximum speed (in 2019) and for peak hour conditions. The peak hour conditions were estimated using a data-driven function using data like the intensity on the road, time of the day, no. of lanes, etc. (See the speed_network_data notebook for all details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 NetworkX graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, there was some missing data about e.g. the maximum speed on the road sections. This data was filled by looking at the maximum speed of adjacent road sections. This was done by creating a directed NetworkX graph. \n",
    "\n",
    "Beneath, you see the code that was used to create the graph. The start and end point of each edge was added as a node to the network. The road section was then added as an edge between the two nodes. Several attributes were added to the edges to keep track of the properties of the road section.\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    nx.set_edge_attributes(G, 0, 'Max_speed')\n",
    "    nx.set_edge_attributes(G, 0, 'Road_number')\n",
    "    nx.set_edge_attributes(G, 0, 'Road_section_id')\n",
    "\n",
    "    for i in range(len(merged_network)):\n",
    "\n",
    "        # Get the first and last node from each road section\n",
    "        start = merged_network.geometry[i].coords[0]\n",
    "        end = merged_network.geometry[i].coords[-1]\n",
    "\n",
    "        # Add the nodes and edges to the graph\n",
    "        G.add_node(start)\n",
    "        G.add_node(end)\n",
    "\n",
    "        G.add_edge(start, end, geometry=merged_network.geometry[i])\n",
    "\n",
    "        # Add different attributes from the dataframe to the graph\n",
    "        G.edges[(start, end)]['Max_speed'] = merged_network.Max_speed[i]\n",
    "        G.edges[(start, end)]['Road_number'] = merged_network.Road_number[i]\n",
    "        G.edges[(start, end)]['Road_section_id'] = merged_network.Road_section_id[i]\n",
    "\n",
    "Because of NetworkX functions, it was easy to get adjecent edges tp use for filling in missing data. Later on, more attributes were added to the network as more was calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load networkX graph\n",
    "G = pickle.load(open('NetworkX_graph.pickle', 'rb'))\n",
    "\n",
    "# Show edge attributes of random edge\n",
    "G.edges[list(G.edges)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the graph contains speeds for optimal and peak hour conditions, road number, road section id and the travel times.\n",
    "\n",
    "However, when trying to find the path between shortest nodes, it was quickly concluded that this graph is not yet strongly connected. In other words, it was not possible to find a path between each two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_strongly_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to make the graph fully connected, so it is possible to calculate the travel time between all nodes. The full process of connecting all nodes can be found in the following notebook: **connect_networkX_graph.ipynb** or **HTML VERSION**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect the full network, two different algorithms were used.\n",
    "\n",
    "First, there was looped over each node in graph G to check if the node missed an incoming or outgoing node. If one of those were missing, the algorithms would loop over the nearest nodes and check if it complied to several constraints. If so, the nodes were connected. Using this method around 1800 edges were added to the network and all nodes had at least an incoming and an outgoing edge. (So it is possible to reach and leave the node).\n",
    "\n",
    "After that, the network was still not strongly connected and a second algorithm was used. There were still over 400 strongly connected components (instead of 1). The second algorithm looped over all strongly connected components and would connect each component to the nearest component. This algorithm used less constraints and the added edges might be less accurate.\n",
    "\n",
    "Still, after the second algorithm, the whole network was strongly connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load networkX graph\n",
    "G = pickle.load(open('NetworkX_graph_new.pickle', 'rb'))\n",
    "\n",
    "# Check if new network is strongly connected\n",
    "nx.is_strongly_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there only needs to be checked if edges that were added to the network seem realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about when the edge was added (or if it already existed)\n",
    "new_edge_val = np.array(list(nx.get_edge_attributes(G, 'New_edge').values()))\n",
    "new_edge_key = np.array(list(nx.get_edge_attributes(G, 'New_edge').keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_sorted = []\n",
    "\n",
    "for i in range(3):\n",
    "    edges_sorted.append(new_edge_key[new_edge_val == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Road network of the Netherlands')\n",
    "\n",
    "# Give axis equal scale\n",
    "fig.set_figheight(9)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Define colors and labels\n",
    "colors = ['cornflowerblue', 'red', 'green']\n",
    "labels = ['Original edges', 'Edges added using first method',\n",
    "          'Edges added using second method']\n",
    "\n",
    "# Plot the different colored edges in the road network\n",
    "for i in range(3):\n",
    "    ax.plot([],[], color=colors[i], label=labels[i])\n",
    "\n",
    "    for j in range(len(edges_sorted[i])):\n",
    "        ax.plot(edges_sorted[i][j,:,0], \n",
    "                edges_sorted[i][j,:,1], color=colors[i])\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the edges added in the first iteration seem to connect small stretches between nodes.'\n",
    "\n",
    "The green edges are larger, especially around several roads in Zeeland and Drenthe. This is due to the fact these are more regional roads and were given in the shapefile as a single lane, instead of several lanes as was done for the highways. This means according to the networkX graph, the road could only be driven in one direction and that had to be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Travel time between nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the network graph is finished, a function must be defined to calculate travel times. This function is used in all the algorithms to calculate the travel times. For the full notebook of this function see **df.ipynb** or **HTML VERSION**. This notebook also shows several visualizations of shortest paths between nodes and some visualizations about the areas an inspector serves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_time function\n",
    "def travel_time_func(point1, point2, time='min'):\n",
    "    \"\"\"This function uses the information given in network X to return the travel time between two points.\n",
    "        point1 and point2 should be tuples with the coordinates in longitude, latitude.\n",
    "        if time = 'peak', the peak travel time is used. In all other cases the minimum travel time is used.\"\"\"\n",
    "\n",
    "    # Determine which travel times to use\n",
    "    if time == 'peak':\n",
    "        time_string = 'Peak_travel_time_[s]'\n",
    "    else:\n",
    "        time_string = 'Min_travel_time_[s]'\n",
    "\n",
    "    # Change points to Dutch system\n",
    "    p1_x, p1_y = WGS84toDutchRD(point1[0], point1[1]) # inspector\n",
    "    p2_x, p2_y = WGS84toDutchRD(point2[0], point2[1]) # incident\n",
    "\n",
    "    # Create numpy matrix from nodes\n",
    "    A = np.array(list(G.nodes()))\n",
    "\n",
    "    # Get node closest to each point\n",
    "    dist_node1, index_node1 = spatial.KDTree(A).query([p1_x, p1_y])\n",
    "    node1 = (A[index_node1][0], A[index_node1][1])\n",
    "\n",
    "    dist_node2, index_node2 = spatial.KDTree(A).query([p2_x, p2_y])\n",
    "    node2 = (A[index_node2][0], A[index_node2][1])\n",
    "\n",
    "    # Get shortest path between nodes\n",
    "    route = nx.shortest_path(G, node1, node2, time_string)\n",
    "    travel_time = nx.shortest_path_length(G, node1, node2, time_string)\n",
    "\n",
    "    return route, travel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function needs as input the WGS84 coordinates of the inspector (origin) and the incident (destination). It then searches for the node closest to the inspector and the node closest to the incident. After that, the shortest path between those 2 nodes is calculated based, using the travel time in optimal conditions, or if you prefer, the travel time using peak hour conditions. \n",
    "\n",
    "Because the distances between 2 nodes are often short (most of the time it takes only a few seconds to travel between two adjacent nodes), the distance between the actual locations and the nodes is neglected. In the aforementioned notebook, the travel times between the node and each incident was calculated (assuming a speed of 120 km/h and eucledian distance), and on average it took 9.1 seconds, while the median was 3.4 seconds. This is not much time when looking at an average travel time of 18 minutes.\n",
    " \n",
    "(There are several larger outliers however, where the distance between a node and incident are up to 6 minutes. This makes this method less accurate, because the actual travel time between the incident and the inspector could be 6 minutes longer or shorter. But because the average travel time is still many times lower, those outliers will be neglected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithms for inspector locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the 4 different algorithms are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Objective functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Add explanations about which objective function is used, or some other like that. Anyway, make it clear how the results of the algorithms can be compared.***\n",
    "\n",
    "The objective function is to compare the number of road inspectors within a time limit, given a fixed mean travel time, the less the road inspectors, the better the method is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 K-means clustering by distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm of K-means clustering by distance uses the incidents data and the network information as the input. \n",
    "\n",
    "The following steps explain how this method work:\n",
    "\n",
    "(1) The algorithm uses the Euler distance to cluster all the incidents. The number of road inspectors are straightly linked to the number of clusters, one cluster has one road inspector. The initial value of k is define as 120 in this algorithm.\n",
    "\n",
    "(2) With the obtained cluster centers, find the nearest node of the road network. The NetworkX and a method of kd-tree is used to find the nearest network node for each cluster center. The found nodes are considered as the locations of the road inspectors.\n",
    "\n",
    "(3) Calculate the mean travel time of this scenario, check if the mean travel time is within the time limit. If so, minus one to the number of clusters (a.k.a the number of road inspectors).\n",
    "\n",
    "(4) Repeat step 1 to step 3 until the mean travel time exceeds the time limit. Then the scenario before the scenario in which the iteration stop is considered as the optimal scenario.\n",
    "\n",
    "The result of this algorithm is saved to a .html file named \"min_inspector_map.html\", a single run of this algorithm could take around 5 hours. For further studying purpose, the travel time in peak hours are also used to find the optimal locations of road inspectors, the result is also saved to a .html file. The result of locations of road inspectors during peak hours can be refered in 'peak_inspector_map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of converted coordinates\n",
    "converted_coordinates = [DutchRDtoWGS84(rd_x, rd_y) for (rd_x, rd_y) in list(G.nodes)]\n",
    "\n",
    "# Create a Pandas DataFrame from the converted coordinates\n",
    "node_df = pd.DataFrame(converted_coordinates, columns=['Latitude', 'Longitude'])\n",
    "\n",
    "# Display the DataFrame\n",
    "node_df\n",
    "\n",
    "# Define a data structure to store kd-tree nodes\n",
    "KdNode = namedtuple(\"KdNode\", \"point left right\")\n",
    "\n",
    "# Build k-d tree\n",
    "def kdtree(point_list, depth=0):\n",
    "\n",
    "    if not point_list:\n",
    "        return None\n",
    "\n",
    "    k = 2  # 2-dimensional space\n",
    "    axis = depth % k\n",
    "\n",
    "    point_list.sort(key=lambda x: x[axis])\n",
    "    median = len(point_list) // 2\n",
    "\n",
    "    return KdNode(\n",
    "        point=point_list[median],\n",
    "        left=kdtree(point_list[:median], depth + 1),\n",
    "        right=kdtree(point_list[median + 1 :], depth + 1),\n",
    "    )\n",
    "\n",
    "\n",
    "# Find the closest point\n",
    "def closest_node(root, target, depth=0, best=None):\n",
    "    if root is None:\n",
    "        return best\n",
    "\n",
    "    k = 2  # 2-dimensional space\n",
    "    axis = depth % k\n",
    "\n",
    "    next_best = None\n",
    "    next_branch = None\n",
    "\n",
    "    if best is None or (\n",
    "        point_distance(root.point, target) < point_distance(best, target)\n",
    "    ):\n",
    "        next_best = root.point\n",
    "    else:\n",
    "        next_best = best\n",
    "\n",
    "    if target[axis] < root.point[axis]:\n",
    "        next_branch = root.left\n",
    "    else:\n",
    "        next_branch = root.right\n",
    "\n",
    "    return closest_node(next_branch, target, depth + 1, next_best)\n",
    "\n",
    "\n",
    "# Calculate the distance between points\n",
    "def point_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "\n",
    "# Extract points from dataframe\n",
    "points = [(row[0], row[1]) for index, row in node_df.iterrows()]\n",
    "tree = kdtree(points)\n",
    "\n",
    "\n",
    "def clustering_opt(k_value, data, target_travel_time):\n",
    "    # Create a K-Means clustering model\n",
    "    kmeans = KMeans(n_clusters=k_value)\n",
    "\n",
    "    # Fit the model to the latitude and longitude data\n",
    "    locations = data[[\"latitude\", \"longitude\"]].values\n",
    "    kmeans.fit(locations)\n",
    "\n",
    "    # Assign cluster labels to data points\n",
    "    data[\"cluster\"] = kmeans.labels_ + 1\n",
    "\n",
    "    m = folium.Map(\n",
    "        location=[np.mean(locations[:, 0]), np.mean(locations[:, 1])],\n",
    "        zoom_start=8,\n",
    "        zoom_control=False,\n",
    "    )\n",
    "\n",
    "    # Create a MarkerCluster layer for clustered data\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    # Create a dictionary to keep track of the event count in each cluster\n",
    "    cluster_event_count = {}\n",
    "\n",
    "    # Create an array to store node coordinates for each cluster\n",
    "    cluster_coords = []\n",
    "\n",
    "    # Initiate the longest travel time\n",
    "    longest_travel_time = -1  # second\n",
    "\n",
    "    # Initiate the total travel time\n",
    "    total_travel_time = -1  # second\n",
    "\n",
    "    # Initiate the point count\n",
    "    point_count = 0\n",
    "\n",
    "    # Add markers for clustering\n",
    "    for cluster_label in range(1, k_value + 1):\n",
    "        cluster_data = data[data[\"cluster\"] == cluster_label]\n",
    "        cluster_count = len(cluster_data)\n",
    "        cluster_event_count[cluster_label] = cluster_count\n",
    "        cluster_coords.append(cluster_data)\n",
    "\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            popup_text = f\"Cluster: {row['cluster']}<br>Type: {row['type']}<br>Index: {row['index']}\"\n",
    "            folium.Marker(\n",
    "                [row[\"latitude\"], row[\"longitude\"]], icon=None, popup=popup_text\n",
    "            ).add_to(marker_cluster)\n",
    "\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    ins_loc = [closest_node(tree, coord) for coord in cluster_centers]\n",
    "\n",
    "    for i, center in enumerate(ins_loc):\n",
    "        cluster_label = i + 1\n",
    "        center_popup_text = f\"Cluster_No.{i + 1}<br>Event_Count:{cluster_event_count[cluster_label]} \\\n",
    "            <br>Coordinate:{[format(center[0], '3f'), format(center[1], '.3f')]}\"\n",
    "\n",
    "        # Use ClickForMarker to display the event count when clicking the cluster center\n",
    "        folium.ClickForMarker(popup=center_popup_text).add_to(m)\n",
    "\n",
    "        folium.Marker(\n",
    "            [center[0], center[1]],\n",
    "            icon=folium.Icon(color=\"red\"),\n",
    "            popup=center_popup_text,\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Initialize a list to store time-related information\n",
    "    time_data = []\n",
    "\n",
    "    # Calculate and update the longest travel time in each cluster\n",
    "    for i, center in enumerate(ins_loc):\n",
    "        cluster_label = i + 1\n",
    "        cluster_data = data[data[\"cluster\"] == cluster_label]\n",
    "        point1 = (ins_loc[i][1], ins_loc[i][0])\n",
    "\n",
    "        for index, row in cluster_data.iterrows():\n",
    "            point2 = (row[\"longitude\"], row[\"latitude\"])\n",
    "            r, time_this = travel_time_func(point1, point2, time='min')\n",
    "            total_travel_time += time_this\n",
    "\n",
    "            if time_this > target_travel_time:\n",
    "                point_count += 1\n",
    "\n",
    "            # Store time-related information in the time_data list\n",
    "            time_data.append(\n",
    "                [\n",
    "                    row[\"index\"],\n",
    "                    row[\"id\"],\n",
    "                    row[\"type\"],\n",
    "                    row[\"longitude\"],\n",
    "                    row[\"latitude\"],\n",
    "                    cluster_label,\n",
    "                    time_this,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # Create a Pandas DataFrame to store time-related information\n",
    "    time_df = pd.DataFrame(\n",
    "        time_data,\n",
    "        columns=[\n",
    "            \"index\",\n",
    "            \"id\",\n",
    "            \"type\",\n",
    "            \"longitude\",\n",
    "            \"latitude\",\n",
    "            \"cluster\",\n",
    "            \"time_this\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Calculat the mean travel time\n",
    "    incident_count = data.shape[0]\n",
    "    mean_travel_time = total_travel_time / incident_count\n",
    "\n",
    "    # Print the longest travel time of the scenario with specific number of inspectors\n",
    "    print(f\"The mean travel time is {mean_travel_time} seconds!\")\n",
    "\n",
    "    return m, ins_loc, mean_travel_time, point_count, time_df\n",
    "\n",
    "\n",
    "def find_optimal(initial_k_value, target_travel_time):\n",
    "    # The unit of travel time is second\n",
    "    k_value = initial_k_value\n",
    "\n",
    "    # Initialize the travel time\n",
    "    flag = True\n",
    "\n",
    "    # Iterate to find the optimal number\n",
    "    while True:\n",
    "        if flag == False:\n",
    "            # Keep a record for the previous optimised results\n",
    "            opt_clustered_map = clustered_map\n",
    "            opt_inspector_locations = inspector_locations\n",
    "            opt_mean_travel_time = mean_travel_time\n",
    "            travel_times = time\n",
    "\n",
    "        # Calculate the travel time and the clustered map\n",
    "        (\n",
    "            clustered_map,\n",
    "            inspector_locations,\n",
    "            mean_travel_time,\n",
    "            point_count,\n",
    "            time,\n",
    "        ) = clustering_opt(k_value, incidents_df, target_travel_time)\n",
    "        flag = False\n",
    "        k_value -= 1\n",
    "        if mean_travel_time > target_travel_time:\n",
    "            break\n",
    "\n",
    "    optimal_number = k_value + 1\n",
    "    print(f\"The optimal number of inspectors is {optimal_number}!\\n\")\n",
    "\n",
    "    return (\n",
    "        opt_clustered_map,\n",
    "        opt_inspector_locations,\n",
    "        opt_mean_travel_time,\n",
    "        optimal_number,\n",
    "        travel_times,\n",
    "    )\n",
    "\n",
    "\n",
    "# Use the function to calculate the optimal locations of road inspectors\n",
    "(\n",
    "    inspector_map,\n",
    "    inspector_locations,\n",
    "    opt_mean_travel_time,\n",
    "    optimal_number,\n",
    "    travel_times,\n",
    ") = find_optimal(50, 1080)\n",
    "\n",
    "# The result is saved to a .html file, a single run could take around 5 hours. Here the .html is read to illustarte the result.\n",
    "# The codes below save the result to a .html file named \"min_inspector_map.html\"\n",
    "inspector_map.save(\"min_inspector_map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 K-means clustering by travel time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is simliar with the first algorithm, which also use K means method to cluster. The main difference is that we calculate the travel time as the criteria to cluster instead of using distance directly. The following step shows how algorithm works:\n",
    "\n",
    "(1) Set the number of inspector is 47, which is same as the first algorthim. \n",
    "\n",
    "(2) Intialize 47 cluster center by using traditional K means cluster method (by distance)\n",
    "\n",
    "(3) Set the criteria as 18 mins, and calculate travel time between all incidents within one cluster and corresponding the cluster center\n",
    "\n",
    "(4) Determine the maximum of travel time for each cluster, if any travel time exceeds 18 minutes, all cluster center need to be relocated\n",
    "\n",
    "(5) Iterate to find the location of cluster centers until no redrawing process existing\n",
    "\n",
    "The location for optimal result is shown in **'JOEY.csv'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree  # Using cKDTree from scipy\n",
    "import pandas as pd\n",
    "\n",
    "# Build KD Tree\n",
    "def build_kd_tree(node_data):\n",
    "    return cKDTree(node_data)\n",
    "\n",
    "# Find the closest point\n",
    "def closest_node(kdtree, target):\n",
    "    _, nearest_node_index = kdtree.query(target, k=1)\n",
    "    return kdtree.data[nearest_node_index]\n",
    "\n",
    "# Custom K-means function\n",
    "def custom_kmeans_with_travel_time(data, k, max_iterations=5):\n",
    "    # Build KD Tree\n",
    "    kdtree = build_kd_tree(node_data)\n",
    "\n",
    "    # Initialize K-means\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=max_iterations, n_init=1, random_state=0)\n",
    "\n",
    "    # Fit K-means to get initial cluster centers\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # Initialize cluster_centers with initial cluster centers\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_centers = cluster_centers[:, [1, 0]]\n",
    "\n",
    "    max_travel_time = 18 * 60  # 18 minutes in seconds\n",
    "\n",
    "    # Continue reassigning incidents until no reassignments are made (Here is the main difference from the first method!)\n",
    "    reassignments_made = True\n",
    "    while reassignments_made:\n",
    "        reassignments_made = False\n",
    "        for i in range(k):\n",
    "            cluster_center = cluster_centers[i]\n",
    "            cluster_incidents = data[kmeans.labels_ == i]\n",
    "            cluster_incidents = cluster_incidents[:, [1, 0]]\n",
    "\n",
    "            for incident in cluster_incidents:\n",
    "                _, travel_time = travel_time_func(incident, cluster_center)\n",
    "                if travel_time > max_travel_time:\n",
    "                    # Find the nearest node using KD Tree\n",
    "                    nearest_node = closest_node(kdtree, incident)\n",
    "\n",
    "                    # Reassign the incident to the nearest cluster\n",
    "                    cluster_centers[i] = nearest_node\n",
    "                    reassignments_made = True\n",
    "                    break\n",
    "            \n",
    "        print(f'cluster center {i} is {cluster_centers}')          \n",
    "\n",
    "    return kmeans.labels_, cluster_centers\n",
    "\n",
    "# Use the new KD Tree library\n",
    "k = 47\n",
    "data = selected_data\n",
    "node_data = node_df[['longitude', 'latitude']].values\n",
    "cluster_labels, cluster_centers = custom_kmeans_with_travel_time(data, k, max_iterations=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulated annealing algorithm is applied to build a machine learning optimizer in this part. The explanations of the model are as follows:\n",
    "\n",
    " - Goal:\n",
    "\n",
    "      Try to find the optimal locations of inspectors on the network with a minmum average travel time from each accident point to the nearest inpector's location.\n",
    "\n",
    " - Input:\n",
    "\n",
    "      (1) Possible locations of inspectors(the possible locations refer to all the nodes on NetworkX)\n",
    "\n",
    "      (2) Filtered accident positions(in order to have a much more efficient model, the train set with 80% num of total accident points is filtered to give 100 clustering center of accidents, using K-means clustering by distance method)\n",
    "\n",
    "      (3) travel time function(the function defined previously)\n",
    "  \n",
    "      (4) Cooling rate, initial temperature(they control the speed of convergence and thus the possiblity to avoid sub minimum point), max num of iterations\n",
    " \n",
    "  - Output:\n",
    "\n",
    "      (1) Optimal distribution of 47 inspectors' locations\n",
    "\n",
    "\n",
    " - Step:\n",
    "    \n",
    "      (1) Determine the 100 clustering centers of accidents using K-means clustering by distance method\n",
    "\n",
    "      (2) Set values for cooling rate and initial temperature\n",
    "\n",
    "      (3) Calculate the current result of average travel time\n",
    "\n",
    "      (4) Change random num of current inspectors' locations to new points\n",
    "\n",
    "      (5) Calculate the new result of average travel time\n",
    "\n",
    "      (6) Based on the camparision of the 2 previous results with Metropolis Criterion:\n",
    "\n",
    "      $$P_{\\text{accept}} = \\begin{cases} 1, & \\text{if } \\Delta E \\leq 0 \\\\ e^{-\\frac{\\Delta E}{T}}, & \\text{if } \\Delta E > 0 \\end{cases}$$\n",
    "\n",
    "      it is determined to what extent the new solution should be accepted.\n",
    "\n",
    "      (7) Change the current temperature with the defined cooling rate\n",
    "\n",
    "      (8) Go back to step (3) until iteration meets the max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Frequency-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency-based method is using the several highest frequency as the candidate for the optimisation. In this case 500 road segments with the highest frequency are chosen. After that the Gurobi optimization is set to get the best possible location. The optimization has the objective to minimize travel time to every incidents. The model input is the possible location of the inspectors based on their frequency. Furthermore, decision variables are defined. The binary variables of the inspectors location is stored in the *inspector_open* variable. The assignment of the inspector to each incident is also defined as the binary values in array shape. This is stored in *flow* variable. The process of the frequency-based method is depicted in the flowchart below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](frequency_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment of the incident is done by calculating the distance from the incident point to every road section geometry. The distance is using the euclidean distance based on the incidents and road segments coordinate.\n",
    "\n",
    "The code below is the optimization part of the code. To get a full information of the frequency-based method step through, the **road_inspector_location.ipynb** is referred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are preparation of the model and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the definition of the candidates\n",
    "inspectors = location_list_rank_only(ranked_road_section, road_id_dict,500)\n",
    "\n",
    "inspector_list = [WGS84toDutchRD(inspectors[i][1], inspectors[i][0]) for i in range(len(inspectors))]\n",
    "\n",
    "# the number of sample for the optimization process\n",
    "# the result from old samples is used as the initial solution for the new sample\n",
    "# optimization as the 'warm start'\n",
    "old_sample = 100\n",
    "new_sample = 2000\n",
    "\n",
    "# train data preparation\n",
    "train_list = [WGS84toDutchRD(row.longitude, row.latitude) for idx, row in gdf_incident_wgs84.iterrows()]\n",
    "train_list = train_list[0:new_sample]\n",
    "\n",
    "# number of inspector definition for the optimization\n",
    "ins_num = 47\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "model = gp.Model(name='inspector location optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting binary variables for the chosen inspector\n",
    "inspector_open = model.addVars(inspector_list, vtype=gp.GRB.BINARY, name=\"inspector_open\")\n",
    "\n",
    "# for a warm start: set an initial solution for the next iteration\n",
    "for i in inspector_list:\n",
    "    if i in chosen_inspectors:\n",
    "        inspector_open[i].start = 1\n",
    "    else:\n",
    "        inspector_open[i].start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting binary variables for the assignment of the inspector\n",
    "# to each incident\n",
    "flow = {}\n",
    "for i in train_list:\n",
    "    for j in inspector_list:\n",
    "        flow[i,j] = model.addVar(vtype=gp.GRB.BINARY, name=f\"flow_{i}_{j}\")\n",
    "\n",
    "\n",
    "# for a warm start: set an initial solution for the next iteration\n",
    "for i in train_list[0:old_sample]:\n",
    "    incident_index = train_list.index(i)\n",
    "    for j in inspector_list:\n",
    "        inspector_index = inspector_list.index(j)\n",
    "        if  assigned_flows[incident_index, inspector_index] > 0.5:\n",
    "            flow[i, j].start = 1\n",
    "        else:\n",
    "            flow[i, j].start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of iterations, and threads \n",
    "# to reduce the computational time\n",
    "max_iterations = 50\n",
    "model.Params.IterationLimit = max_iterations\n",
    "model.Params.Threads = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional constraint for this problem is the number of inspector and the incident needs to be assigned to one inspector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the objective function for minimizing the travel time\n",
    "# each incident\n",
    "objective = model.setObjective(gp.quicksum(\n",
    "                                flow[i, j] * travel_time_func_freq(j, i, time='min')\n",
    "                                for i in train_list for j in inspector_list\n",
    "                                ), \n",
    "                                sense=gp.GRB.MINIMIZE)\n",
    "\n",
    "\n",
    "# set constraints\n",
    "accident_constraint = model.addConstrs(gp.quicksum(flow[i, j] for j in inspector_list) == 1 for i in train_list)\n",
    "number_const = model.addConstr(gp.quicksum(inspector_open[i] for i in inspector_list) == ins_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the optimization\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are depicted in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>inspector_longitude</th>\n",
       "      <th>inspector_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.464330</td>\n",
       "      <td>52.114037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.404451</td>\n",
       "      <td>52.077426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.404912</td>\n",
       "      <td>51.470079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.048051</td>\n",
       "      <td>52.111585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.867038</td>\n",
       "      <td>52.416876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.430408</td>\n",
       "      <td>51.410234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5.231189</td>\n",
       "      <td>51.521216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5.183445</td>\n",
       "      <td>51.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4.985770</td>\n",
       "      <td>51.914184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4.731319</td>\n",
       "      <td>52.060078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5.479357</td>\n",
       "      <td>52.165201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4.537489</td>\n",
       "      <td>51.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5.827633</td>\n",
       "      <td>52.026531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4.816565</td>\n",
       "      <td>52.065026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4.986789</td>\n",
       "      <td>52.187305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5.095632</td>\n",
       "      <td>51.981006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4.830527</td>\n",
       "      <td>52.067922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>4.670357</td>\n",
       "      <td>52.438107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>5.140929</td>\n",
       "      <td>51.945572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>4.907971</td>\n",
       "      <td>52.072231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>4.654694</td>\n",
       "      <td>52.022318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>4.705525</td>\n",
       "      <td>52.040203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>4.546976</td>\n",
       "      <td>51.899274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>4.709110</td>\n",
       "      <td>52.042374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>4.617865</td>\n",
       "      <td>51.979827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>4.982492</td>\n",
       "      <td>52.227373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>4.731214</td>\n",
       "      <td>52.296520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>5.480238</td>\n",
       "      <td>52.165253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>4.901743</td>\n",
       "      <td>52.071198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>4.561805</td>\n",
       "      <td>52.166344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>5.020996</td>\n",
       "      <td>51.953294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>4.986765</td>\n",
       "      <td>52.171137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>5.198169</td>\n",
       "      <td>52.092588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>5.797091</td>\n",
       "      <td>51.009008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>4.884440</td>\n",
       "      <td>52.339617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>4.849647</td>\n",
       "      <td>52.401455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>5.095351</td>\n",
       "      <td>51.969643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>4.747886</td>\n",
       "      <td>52.363902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>5.875799</td>\n",
       "      <td>52.025226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>5.672975</td>\n",
       "      <td>52.012722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5.257510</td>\n",
       "      <td>52.105949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>5.157991</td>\n",
       "      <td>52.076872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>4.752857</td>\n",
       "      <td>51.526333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>4.523877</td>\n",
       "      <td>52.147799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>4.965131</td>\n",
       "      <td>52.080664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>4.882668</td>\n",
       "      <td>51.686942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>4.738101</td>\n",
       "      <td>52.064522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  inspector_longitude  inspector_latitude\n",
       "0            0             4.464330           52.114037\n",
       "1            1             4.404451           52.077426\n",
       "2            2             5.404912           51.470079\n",
       "3            3             5.048051           52.111585\n",
       "4            4             4.867038           52.416876\n",
       "5            5             5.430408           51.410234\n",
       "6            6             5.231189           51.521216\n",
       "7            7             5.183445           51.534300\n",
       "8            8             4.985770           51.914184\n",
       "9            9             4.731319           52.060078\n",
       "10          10             5.479357           52.165201\n",
       "11          11             4.537489           51.911134\n",
       "12          12             5.827633           52.026531\n",
       "13          13             4.816565           52.065026\n",
       "14          14             4.986789           52.187305\n",
       "15          15             5.095632           51.981006\n",
       "16          16             4.830527           52.067922\n",
       "17          17             4.670357           52.438107\n",
       "18          18             5.140929           51.945572\n",
       "19          19             4.907971           52.072231\n",
       "20          20             4.654694           52.022318\n",
       "21          21             4.705525           52.040203\n",
       "22          22             4.546976           51.899274\n",
       "23          23             4.709110           52.042374\n",
       "24          24             4.617865           51.979827\n",
       "25          25             4.982492           52.227373\n",
       "26          26             4.731214           52.296520\n",
       "27          27             5.480238           52.165253\n",
       "28          28             4.901743           52.071198\n",
       "29          29             4.561805           52.166344\n",
       "30          30             5.020996           51.953294\n",
       "31          31             4.986765           52.171137\n",
       "32          32             5.198169           52.092588\n",
       "33          33             5.797091           51.009008\n",
       "34          34             4.884440           52.339617\n",
       "35          35             4.849647           52.401455\n",
       "36          36             5.095351           51.969643\n",
       "37          37             4.747886           52.363902\n",
       "38          38             5.875799           52.025226\n",
       "39          39             5.672975           52.012722\n",
       "40          40             5.257510           52.105949\n",
       "41          41             5.157991           52.076872\n",
       "42          42             4.752857           51.526333\n",
       "43          43             4.523877           52.147799\n",
       "44          44             4.965131           52.080664\n",
       "45          45             4.882668           51.686942\n",
       "46          46             4.738101           52.064522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspector_freq = pd.read_csv('inspector_47_frequency')\n",
    "inspector_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>incident_latitude</th>\n",
       "      <th>incident_longitude</th>\n",
       "      <th>inspector_latitude</th>\n",
       "      <th>inspector_longitude</th>\n",
       "      <th>travel_time[s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>52.346931</td>\n",
       "      <td>4.974663</td>\n",
       "      <td>52.042374</td>\n",
       "      <td>4.709110</td>\n",
       "      <td>2044.186394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52.514820</td>\n",
       "      <td>4.716725</td>\n",
       "      <td>51.865601</td>\n",
       "      <td>5.720545</td>\n",
       "      <td>4290.843888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52.609730</td>\n",
       "      <td>4.738364</td>\n",
       "      <td>51.713488</td>\n",
       "      <td>5.341347</td>\n",
       "      <td>3889.921043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>52.204929</td>\n",
       "      <td>6.824692</td>\n",
       "      <td>52.684687</td>\n",
       "      <td>6.269522</td>\n",
       "      <td>4003.598620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>52.041920</td>\n",
       "      <td>4.346407</td>\n",
       "      <td>52.076872</td>\n",
       "      <td>5.157991</td>\n",
       "      <td>2219.686538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57685</th>\n",
       "      <td>57685</td>\n",
       "      <td>51.003658</td>\n",
       "      <td>5.791368</td>\n",
       "      <td>51.408404</td>\n",
       "      <td>5.567596</td>\n",
       "      <td>3383.915146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57686</th>\n",
       "      <td>57686</td>\n",
       "      <td>51.875309</td>\n",
       "      <td>4.372907</td>\n",
       "      <td>51.493147</td>\n",
       "      <td>5.316205</td>\n",
       "      <td>3201.669130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57687</th>\n",
       "      <td>57687</td>\n",
       "      <td>52.244808</td>\n",
       "      <td>6.343580</td>\n",
       "      <td>52.438107</td>\n",
       "      <td>4.670357</td>\n",
       "      <td>6326.814777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57688</th>\n",
       "      <td>57688</td>\n",
       "      <td>52.396221</td>\n",
       "      <td>4.705645</td>\n",
       "      <td>52.178162</td>\n",
       "      <td>5.180296</td>\n",
       "      <td>2582.648111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57689</th>\n",
       "      <td>57689</td>\n",
       "      <td>52.038780</td>\n",
       "      <td>4.351567</td>\n",
       "      <td>51.800999</td>\n",
       "      <td>4.459076</td>\n",
       "      <td>1509.140796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57690 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  incident_latitude  incident_longitude  inspector_latitude  \\\n",
       "0               0          52.346931            4.974663           52.042374   \n",
       "1               1          52.514820            4.716725           51.865601   \n",
       "2               2          52.609730            4.738364           51.713488   \n",
       "3               3          52.204929            6.824692           52.684687   \n",
       "4               4          52.041920            4.346407           52.076872   \n",
       "...           ...                ...                 ...                 ...   \n",
       "57685       57685          51.003658            5.791368           51.408404   \n",
       "57686       57686          51.875309            4.372907           51.493147   \n",
       "57687       57687          52.244808            6.343580           52.438107   \n",
       "57688       57688          52.396221            4.705645           52.178162   \n",
       "57689       57689          52.038780            4.351567           51.800999   \n",
       "\n",
       "       inspector_longitude  travel_time[s]  \n",
       "0                 4.709110     2044.186394  \n",
       "1                 5.720545     4290.843888  \n",
       "2                 5.341347     3889.921043  \n",
       "3                 6.269522     4003.598620  \n",
       "4                 5.157991     2219.686538  \n",
       "...                    ...             ...  \n",
       "57685             5.567596     3383.915146  \n",
       "57686             5.316205     3201.669130  \n",
       "57687             4.670357     6326.814777  \n",
       "57688             5.180296     2582.648111  \n",
       "57689             4.459076     1509.140796  \n",
       "\n",
       "[57690 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident_to_inspector_freq = pd.read_csv('incident_47_inspector_frequency')\n",
    "incident_to_inspector_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey will explain here more :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will show the results from the validation, and thus shows how good the algorithms perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Read and input the inspectors coordinates of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('Joey result1.csv')\n",
    "\n",
    "latitude = result_df['latitude'].values\n",
    "longitude = result_df['longitude'].values\n",
    "\n",
    "latitude = np.array([float(lat) for lat in latitude])\n",
    "longitude = np.array([float(lon) for lon in longitude])\n",
    "\n",
    "inspector_coordinates = np.column_stack((longitude, latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Here we calculate the shorest travel time of each incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = remaining_data\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each incident\n",
    "for i, incident_coord in enumerate(validation_data):\n",
    "    minimum_travel_time = float('inf')\n",
    "    \n",
    "    # Loop through each inspector's coordinates\n",
    "    for j, inspector_coord in enumerate(inspector_coordinates):\n",
    "        _, travel_time = travel_time_func(inspector_coord, incident_coord)\n",
    "        if travel_time < minimum_travel_time:\n",
    "            minimum_travel_time = travel_time\n",
    "    \n",
    "    results.append([i, minimum_travel_time])\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results, columns=[\"Incident Index\", \"Minimum Travel Time\"])\n",
    "start_time_column = pd.DataFrame(validation_dataframe['start_time'].values, columns=['start_time'])\n",
    "longitude_column = pd.DataFrame(validation_dataframe['longitude'].values, columns=['longitude'])\n",
    "latitude_column = pd.DataFrame(validation_dataframe['latitude'].values, columns=['latitude'])\n",
    "results_df['start_time'] = start_time_column\n",
    "results_df['longitude'] = longitude_column\n",
    "results_df['latitude'] = latitude_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Validation in time dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we classify the data by day of week and time of a day, and then calculate the average travel time they need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = results_df['start_time'].dt.dayofweek\n",
    "results_df['day of week'] = day_of_week\n",
    "results_df['hour of day'] = results_df['start_time'].apply(lambda x: x.hour)\n",
    "\n",
    "results = []\n",
    "time_ranges = [(0, 5), (6, 9), (10, 14), (15, 18), (19, 23)]\n",
    "for i in range(7):\n",
    "    data_filter = results_df[results_df['day of week'] == i]\n",
    "\n",
    "    for start_hour, end_hour in time_ranges:\n",
    "        data = data_filter[data_filter['hour of day'].between(start_hour, end_hour)][['Minimum Travel Time']]\n",
    "        \n",
    "        # Calculate the probability that travel time is less than 18 minutes\n",
    "        count_below_18mins = (data['Minimum Travel Time'] < 1080).sum()\n",
    "\n",
    "\n",
    "        if len(data) > 0:\n",
    "            pro_below_18mins = count_below_18mins / len(data)\n",
    "            pro_below_18mins = \"{:.2f}\".format(pro_below_18mins)\n",
    "\n",
    "        else:\n",
    "            pro_below_18mins = 'NAN'\n",
    "\n",
    "        # Calculate the average travel time\n",
    "        avg_travel_time = data['Minimum Travel Time'].mean()\n",
    "\n",
    "\n",
    "        if pd.notna(avg_travel_time):\n",
    "            avg_travel_time = \"{:.2f}\".format(avg_travel_time)\n",
    "        else:\n",
    "            avg_travel_time = 'NAN'\n",
    "\n",
    "        number_of_incidents = len(data)\n",
    "        results.append({\n",
    "            'Day': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][i],\n",
    "            'Time ranges': f'{start_hour}-{end_hour}',\n",
    "            'Number of incidents': number_of_incidents,\n",
    "            'Average Travel Time': avg_travel_time,\n",
    "            'Probability of <18 mins': pro_below_18mins\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Prepare the data\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "time_ranges = ['0-5', '6-9', '10-14', '15-18', '19-23']\n",
    "\n",
    "# Sample data for demonstration purposes\n",
    "Z = np.random.rand(len(time_ranges), len(days))  \n",
    "colors = np.random.rand(len(time_ranges), len(days))  \n",
    "\n",
    "# Create a surface plot\n",
    "surface = go.Surface(z=Z, x=days, y=time_ranges, colorscale='Viridis', cmin=0, cmax=1, colorbar=dict(title='Probability of <18 mins'))\n",
    "\n",
    "# Set axis labels\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Day',\n",
    "        yaxis_title='Time ranges',\n",
    "        zaxis_title='Average Travel Time'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set custom x-axis labels\n",
    "layout.scene.xaxis.update(tickvals=days, ticktext=days)\n",
    "\n",
    "# Set custom y-axis labels\n",
    "layout.scene.yaxis.update(tickvals=time_ranges, ticktext=time_ranges)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[surface], layout=layout)\n",
    "\n",
    "# Set subplot title\n",
    "fig.update_layout(title='Validation Heatmap')\n",
    "\n",
    "# Save the interactive plot as an HTML file\n",
    "\n",
    "# pyo.plot(fig, filename='Time dimension.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Validation in Geographic dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "m = folium.Map(location=[52.399190, 4.893658], zoom_start=10, zoom_control=False)\n",
    "\n",
    "# Create a HeatMap layer to visualize Minimum Travel Time\n",
    "heat_data = [[row['latitude'], row['longitude'], row['Minimum Travel Time']] for _, row in results_df.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "color_gradient = {\n",
    "    0.0: 'blue',\n",
    "    500.0: 'green',\n",
    "    1000.0: 'yellow',\n",
    "    2000.0: 'red'\n",
    "}\n",
    "\n",
    "HeatMap(heat_data, gradient=color_gradient).add_to(m)\n",
    "\n",
    "# m.save('Geographic_dimension.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Show which algorithm is best and why***\n",
    "\n",
    "***Link again to dashboard for more results*** Also refer to notebook(s) that where used to make dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What could have been done better?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) For the K-means method, the clusters are based on the Euler distance. If the clusters can use the on-road travel distance, the locations of road inspectors could be more optimised. This is the next-step optimising direction of the K-means method.\n",
    "2) The incident data could be narrowed down more to reduce the running time of the model.\n",
    "3) The candidate nodes for the inspector location could be defined at the filtering phases. It makes the size of model input smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
